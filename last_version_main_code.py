# -*- coding: utf-8 -*-
"""Neck_classifier_without_stop-Copy1 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TIRpjDhuphpPK_9uAPuvepIM-AlUUVhU
"""

"""
Classification algorithm for neck motion.
Class "1": Forward
Class "2": Right
Class "3": Left
Class "4": Back
Stop was not considered inside the classification process due to the generation of unbalanced data.
"""
#!pip install --upgrade --force-reinstall xlrd
#!pip install --upgrade pandas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn

#Filtering libraries
import numpy as np
from scipy.signal import butter, lfilter, freqz
from scipy import signal
import matplotlib.pyplot as plt
import scipy.signal
#Trendline
from scipy.stats import linregress
from numpy import ones,vstack
from numpy.linalg import lstsq
from sklearn.metrics import mean_squared_error, r2_score
import statistics
#Feature extraction
from scipy.stats import skew,kurtosis
#PCA libraries
import time
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from numpy.fft import fft
import plotly.express as px

#Classification algorithms
from pandas.plotting import scatter_matrix
from matplotlib import cm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn import *
import time
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_validate

from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix

from sklearn.tree import DecisionTreeClassifier

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn import svm
from sklearn.preprocessing import StandardScaler

from sklearn.neural_network import MLPClassifier

# Configuration Butterworth low pass filter
fs = 100  # Sampling frequency
fc = 30  # Cut-off frequency of the filter
w = fc / (fs / 2) # Normalize the frequency
order = 5
columns_PDs=['PD1','PD2']
samples=100
overlap=10
windows_quantity=0
samples_range=1000
labels_uno=[1,2,3,4,1,2,4,3,1,3,2,4,1,3,4,2,1,4,2,3,1,4,3,2]
labels_dos=[2,1,3,4,2,1,4,3,2,3,1,4,2,3,4,1,2,4,1,3,2,4,3,1]
labels_tres=[3,1,2,4,3,1,4,2,3,2,1,4,3,2,4,1,3,4,1,2,3,4,2,1]
labels_cuatro=[4,1,2,3,4,1,3,2,4,2,1,3,4,2,3,1,4,3,1,2,4,3,2,1]

#Only wheelchair sensors data
def process_csv_file (csvdata,labels):
  neck_sensors_data = pd.read_excel(csvdata)  #Read csv data
  neck_sensors_data=neck_sensors_data.drop(neck_sensors_data.index[[0,1]])
  neck_sensors_data.columns =['Date Time', 'PD1','PD2','0','1','samples']
  del(neck_sensors_data['Date Time'])
  del(neck_sensors_data['0'])
  del(neck_sensors_data['1'])
  del(neck_sensors_data['samples'])
  neck_sensors_data.reset_index(drop=True, inplace=True)
  labels_data=pd.DataFrame()
  pds_tot=pd.DataFrame()
  cont=-1
  for j in range(1,neck_sensors_data.shape[0],samples_range):
    start=(samples_range)*cont
    cont += 1
    if (cont > 0):
      #print(start,j,cont)
      val=cont-1
      labels_data=neck_sensors_data.iloc[start:j]
      #print(labels_data,val)
      #print(len(labels))
      labels_data['label'] = labels[val]

      pds_tot = pd.concat([pds_tot,labels_data])
  return pds_tot

#Join complete dataset
uno=process_csv_file("session1.xls",labels_uno)
dos=process_csv_file("session2.xls",labels_dos)
tres=process_csv_file("session3.xls",labels_tres)
cuatro=process_csv_file("session4.xls",labels_cuatro)
largev=pd.concat([uno,dos,tres,cuatro])
largev.shape
print(largev)

labels=largev.label
labels=labels.to_frame()

def plot_dataset(dataframe_sensors):
  dataframe_sensors = dataframe_sensors.astype(str).astype(float)
  dataframe_sensors = dataframe_sensors.astype(float).astype(int)
  numpy_array = dataframe_sensors.to_numpy()
  PD1=numpy_array[:, 0]
  PD2=numpy_array[:, 1]
  t = np.arange(dataframe_sensors.shape[0]) / fs
  return [PD1,PD2]

only_neck_data_filtered=plot_dataset(largev)
only_neck_data_filtered

"""Butterworth filter"""

#Filtering stage
def butter_lowpass_filter(order,data, w):
    b, a = signal.butter(order, w, 'low')
    output_but = signal.filtfilt(b, a, data)
    return output_but


def apply_filter_plot(t,order,PD1_data,PD2_data,w):
  dataPD1_filteredtest1 = butter_lowpass_filter(order,PD1_data, w)
  dataPD2_filteredtest1 = butter_lowpass_filter(order,PD2_data, w)
  return [dataPD1_filteredtest1,dataPD2_filteredtest1]

def butterworth_filter(dataframe_sensors):
  dataframe_sensors = dataframe_sensors.astype(str).astype(float)
  dataframe_sensors = dataframe_sensors.astype(float).astype(int)
  numpy_array = dataframe_sensors.to_numpy()
  PDseat11=numpy_array[:, 0]
  PDseat12=numpy_array[:, 1]
  lines_filtered_seat=apply_filter_plot((np.arange(dataframe_sensors.shape[0]) / fs),order,PDseat11,PDseat12,w)
  print(lines_filtered_seat, type(lines_filtered_seat))
  wheelchair_filtered_PD1= pd.DataFrame(lines_filtered_seat[0])
  wheelchair_filtered_PD2= pd. DataFrame(lines_filtered_seat[1])
  wheelchair_filtered_PDs = pd.concat([wheelchair_filtered_PD1, wheelchair_filtered_PD2], axis=1)
  wheelchair_filtered_PDs.columns=(['PD1','PD2'])
  return wheelchair_filtered_PDs

only_wheelchair_data_filtered=butterworth_filter(largev)
only_wheelchair_data_filtered

"""CAR filter"""

from scipy.signal import savgol_filter

def raw_data(dataframe_sensors):
  dataframe_sensors = dataframe_sensors.astype(str).astype(float)
  dataframe_sensors = dataframe_sensors.astype(float).astype(int)
  numpy_array = dataframe_sensors.to_numpy()
  PDseat11=numpy_array[:, 0]
  PDseat12=numpy_array[:, 1]
  PDseat11 = savgol_filter(PDseat11, 51, 3) # window size 51, polynomial order 3
  PDseat12 = savgol_filter(PDseat12, 51, 3) # window size 51, polynomial order 3
  wheelchair_filtered_PD1= pd.DataFrame(PDseat11)
  wheelchair_filtered_PD2= pd. DataFrame(PDseat12)
  wheelchair_filtered_PDs = pd.concat([wheelchair_filtered_PD1, wheelchair_filtered_PD2], axis=1)
  wheelchair_filtered_PDs.columns=(['PD1','PD2'])
  return wheelchair_filtered_PDs

raw_data_filtered=raw_data(largev)
raw_data_filtered

#Complete dataset without Butterworth filtering stage
totaldfw= pd.DataFrame()
raw_data_filtered= raw_data_filtered.reset_index(drop=True)
labels= labels.reset_index(drop=True)
totaldfw = pd.concat([raw_data_filtered, labels], axis=1)

#Complete dataset with filtering stage
totaldf= pd.DataFrame()
only_wheelchair_data_filtered= only_wheelchair_data_filtered.reset_index(drop=True)
labels= labels.reset_index(drop=True)
totaldf = pd.concat([only_wheelchair_data_filtered, labels], axis=1)

"""Feature extraction all parameters"""

import math
def Hjorth_param(dataframe):
  out = np.power(dataframe, 2)
  m0= math.sqrt(out.sum())   #Energy-activity
  diff_first=dataframe.diff()
  out2 = np.power(diff_first, 2)
  m2=math.sqrt((1/dataframe.shape[0])*(out2.sum()))
  mobility=math.sqrt(m2/m0) #Second feature
  diff_sec=diff_first.diff()
  out3 = np.power(diff_sec, 2)
  m4=math.sqrt((1/dataframe.shape[0])*(out3.sum()))
  complexity=math.sqrt(m4/m2)   #Third feature
  sparseness= ((m0)/((math.sqrt(m0-m4))*(math.sqrt(m0-m2))))  #Fourth feature
  irregularity=math.sqrt((m2**2)/(m0*m4))   #Fifth feature
  return(m0,mobility,complexity,sparseness,irregularity)

def features(dataframe,meanlabel):
  var_time = np.var(dataframe) #Variance time domain
  rms_time = np.sqrt(np.mean(dataframe**2)) # RMS time domain
  freq_values = fft(dataframe)
  var_freq = np.var(freq_values) #Variance
  rms_freq = abs(np.sqrt(np.mean(freq_values**2))) # RMS
  H_feat=Hjorth_param(dataframe)
  both_feat=[H_feat[0],H_feat[1],H_feat[2],H_feat[3],H_feat[4],var_time,rms_time,var_freq,rms_freq,meanlabel]
  hjorth_feat=[H_feat[0],H_feat[1],H_feat[2],H_feat[3],H_feat[4],meanlabel]
  var_rms_feat=[var_time,rms_time,var_freq,rms_freq,meanlabel]
  return hjorth_feat

###Time windows without overlap
def windows_generation(dataframe_filtered):
  W=[]
  featuresbywindow=[]
  featuresbyPDlist=[]
  W=[dataframe_filtered.iloc[0:samples]]
  for i in range(1,int(dataframe_filtered.shape[0]/samples)):
    W.append(dataframe_filtered.iloc[(i*samples)+1:(i+1)*samples])
  for i in range(len(W)): #Window 0 to N window
    dffeatures=pd.DataFrame(W[i])
    meanlabel = round(np.mean(dffeatures['label']))
    del(dffeatures['label'])
    for photodetectors in dffeatures:
      dataPD=dffeatures[photodetectors]
      featuresbyPD=features(dataPD,meanlabel)
      featuresbyPDlist.append(featuresbyPD)
      #print(featuresbyPDlist,len(featuresbyPDlist))
      if len(featuresbyPDlist) == 2:
        featuresbywindow.append(featuresbyPDlist)
        featuresbyPDlist=[]
  featuresbywindowdf=pd.DataFrame (featuresbywindow)
  return featuresbywindowdf

def windows_generation_overlap(dataframe_filtered):
  W=[]
  featuresbywindow=[]
  featuresbyPDlist=[]
  windows_quantity=int(((dataframe_filtered.shape[0]-samples)/overlap)-1)#-1?
  var=0
  W=[dataframe_filtered.iloc[var:samples-1]]
  df_all_PDs=pd.DataFrame()
  for i in range(windows_quantity):
      var += overlap
      W.append(dataframe_filtered.iloc[var:(var+samples)-1])
  for i in range(len(W)): #Window 0 to N window
    dffeatures=pd.DataFrame(W[i])
    meanlabel = round(np.mean(dffeatures['label']))
    del(dffeatures['label'])
    for photodetectors in dffeatures:
      dataPD=dffeatures[photodetectors]
      featuresbyPD=features(dataPD,meanlabel)
      featuresbyPDlist.append(featuresbyPD)
      print(featuresbyPDlist,len(featuresbyPDlist))
      if len(featuresbyPDlist) == dffeatures.shape[1]:
        feat_test=features_by_line(featuresbyPDlist)
        #df_all_PDs = pd.concat([df_all_PDs, pd.DataFrame(feat_test,columns=['var_timePD1','rms_timePD1','var_freqPD1','rms_freqPD1','labelPD1','var_timePD2','rms_timePD2','var_freqPD2','rms_freqPD2','labels'])], ignore_index=True)
        #df_all_PDs = pd.concat([df_all_PDs, pd.DataFrame(feat_test,columns=['activity1','mobility1','complexity1','sparseness1','irregularity1','var_timePD1','rms_timePD1','var_freqPD1','rms_freqPD1','labelPD1','activity2','mobility2','complexity2','sparseness2','irregularity2','var_timePD2','rms_timePD2','var_freqPD2','rms_freqPD2','labels'])], ignore_index=True)
        df_all_PDs = pd.concat([df_all_PDs, pd.DataFrame(feat_test,columns=['activity1','mobility1','complexity1','sparseness1','irregularity1','labelPD1','activity2','mobility2','complexity2','sparseness2','irregularity2','labels'])], ignore_index=True)

        featuresbyPDlist=[]
  return df_all_PDs

def features_by_line(listbyline):
  complete_line=[]
  matrix=[]
  for i in listbyline:    #Join all features in a list
    for j in i:
      complete_line.append(j)
  matrix.append(complete_line)    #Converts the list as matrix for the dataframe input
  return matrix

featneck=windows_generation_overlap(totaldfw) #allfeaturesallwindows
featneck

del(featneck['labelPD1'])
dftot=featneck

X = dftot.drop(columns=['labels'])
y = dftot['labels']

df=pd.DataFrame(X)
scaler=StandardScaler()
scaler.fit(df)
scaled_data=scaler.transform(df)
pca=PCA(n_components=5)
pca.fit(scaled_data)
x_pca=pca.transform(scaled_data)
print(pca.explained_variance_ratio_)

#fig = px.scatter(x_pca, x=0, y=1, color=y)
#fig.show()

X = dftot.drop(columns=['labels'])
y = dftot['labels']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#KNN

start_time = time.time()
# K value set
classifer = KNeighborsClassifier()
# model training
classifer.fit(X_train,y_train)
print("--- %s model ---" % (time.time() - start_time))
# testing the model
start_time = time.time()
y_pred= classifer.predict(X_test)
print("--- %s predict ---" % (time.time() - start_time))
#print("X_test: ",X_test)
# importing accuracy_score


# printing accuracy
print("Accuracy: ",accuracy_score(y_test,y_pred))


accuracies = cross_val_score(estimator = classifer, X = X_train, y = y_train, cv = 5,scoring="accuracy")
print("accuracies",accuracies)



myscore = make_scorer(roc_auc_score, multi_class='ovo',needs_proba=True)
#print(f1_score(y_test,y_pred,average='micro'))

roc_auc = cross_val_score(estimator = classifer, X = X_train, y = y_train, cv = 5,scoring=myscore)
print("roc_auc",roc_auc)

recall = cross_val_score(estimator = classifer, X = X_train, y = y_train, cv = 5,scoring="recall_macro")
print("recall",recall)

balanced_accuracy = cross_val_score(estimator = classifer, X = X_train, y = y_train, cv = 5,scoring="balanced_accuracy")
print("balanced_accuracy",balanced_accuracy)
print("Accuracy cross validation: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))


cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy = float(cm.diagonal().sum())/len(y_test)
print("\nAccuracy Of kNN For The Given Dataset : ", accuracy)

accuracy_score(y_test,y_pred)

# finding the whole report


print(classification_report(y_test, y_pred))

y_pred_proba= classifer.predict_proba(X_test)

print("y_pred: ",y_pred_proba)
print(roc_auc_score(y_test,y_pred_proba,multi_class='ovr'))



y_pred = cross_val_predict(classifer, X_train, y_train, cv=5)
conf_mat = confusion_matrix(y_train, y_pred)

#Decision tree

start_time = time.time()
clf = DecisionTreeClassifier().fit(X_train, y_train)
print("--- %s model ---" % (time.time() - start_time))
# testing the model
start_time = time.time()
y_pred= clf.predict(X_test)
print("--- %s predict ---" % (time.time() - start_time))


# printing accuracy
print("Accuracy: ",accuracy_score(y_test,y_pred))


accuracies = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="accuracy")
print("accuracies",accuracies)



myscore = make_scorer(roc_auc_score, multi_class='ovo',needs_proba=True)
#print(f1_score(y_test,y_pred,average='micro'))

roc_auc = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring=myscore)
print("roc_auc",roc_auc)
balanced_accuracy = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="balanced_accuracy")
print("balanced_accuracy",balanced_accuracy)

recall = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="recall_macro")
print("recall",recall)

print("Accuracy cross validation: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy = float(cm.diagonal().sum())/len(y_test)
print("\nAccuracy Of kNN For The Given Dataset : ", accuracy)

accuracy_score(y_test,y_pred)

print(classification_report(y_test, y_pred))

y_pred_proba= clf.predict_proba(X_test)

print("y_pred: ",y_pred_proba)
print(roc_auc_score(y_test,y_pred_proba,multi_class='ovr'))

"""time and frequency domain features"""

#Linear Discriminant Analysis

start_time = time.time()
clf = LinearDiscriminantAnalysis(solver='lsqr').fit(X_train, y_train)
print("--- %s model ---" % (time.time() - start_time))
# testing the model
start_time = time.time()
y_pred= clf.predict(X_test)
print("--- %s predict ---" % (time.time() - start_time))


# printing accuracy
print("Accuracy: ",accuracy_score(y_test,y_pred))


accuracies = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="accuracy")
print("accuracies",accuracies)



myscore = make_scorer(roc_auc_score, multi_class='ovo',needs_proba=True)
#print(f1_score(y_test,y_pred,average='micro'))

roc_auc = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring=myscore)
print("roc_auc",roc_auc)
balanced_accuracy = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="balanced_accuracy")
print("balanced_accuracy",balanced_accuracy)

recall = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="recall_macro")
print("recall",recall)

print("Accuracy cross validation: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy = float(cm.diagonal().sum())/len(y_test)
print("\nAccuracy Of LDA For The Given Dataset : ", accuracy)

accuracy_score(y_test,y_pred)

print(classification_report(y_test, y_pred))

y_pred_proba= clf.predict_proba(X_test)

print("y_pred: ",y_pred_proba)
print(roc_auc_score(y_test,y_pred_proba,multi_class='ovr'))

#Support Vector Machine

C=1
tol=1e-3
max_passes=-1
sigma=1

#clf = svm.SVC(C=C,kernel='rbf', tol=tol, max_iter=max_passes)

start_time = time.time()
clf = svm.SVC(C=C,kernel='poly',gamma='scale',probability=True, random_state=0)
#clf = svm.SVC(C=1.0) #indica se a margem é rigida ou suave
clf.fit(X_train,y_train)
print("--- %s model ---" % (time.time() - start_time))
start_time = time.time()
#predição,  o X teste que entrou e ele retorna o y das classes preditas
y_pred = clf.predict(X_test)
print("--- %s predict ---" % (time.time() - start_time))


# printing accuracy
print("Accuracy: ",accuracy_score(y_test,y_pred))


accuracies = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="accuracy")
print("accuracies",accuracies)


myscore = make_scorer(roc_auc_score, multi_class='ovo',needs_proba=True)
#print(f1_score(y_test,y_pred,average='micro'))

roc_auc = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring=myscore)
print("roc_auc",roc_auc)
balanced_accuracy = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="balanced_accuracy")
print("balanced_accuracy",balanced_accuracy)

recall = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="recall_macro")
print("recall",recall)

print("Accuracy cross validation: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy = float(cm.diagonal().sum())/len(y_test)
print("\nAccuracy Of SVM For The Given Dataset : ", accuracy)

accuracy_score(y_test,y_pred)

print(classification_report(y_test, y_pred))

y_pred_proba= clf.predict_proba(X_test)

print("y_pred: ",y_pred_proba)
print(roc_auc_score(y_test,y_pred_proba,multi_class='ovr'))

# rbf 0.7095
#linear 0.64
#sigmoid 0.18
#poly 0.725

# Create an MLP classifier
start_time = time.time()
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)
# Train the MLP classifier
mlp.fit(X_train,y_train)
print("--- %s model ---" % (time.time() - start_time))

start_time = time.time()
y_pred = mlp.predict(X_test)
print("--- %s predict ---" % (time.time() - start_time))
print("Accuracy: ",accuracy_score(y_test,y_pred))
accuracies = cross_val_score(estimator = mlp, X = X_train, y = y_train, cv = 5,scoring="accuracy")
print("accuracies",accuracies)
myscore = make_scorer(roc_auc_score, multi_class='ovo',needs_proba=True)
roc_auc = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring=myscore)
print("roc_auc",roc_auc)
balanced_accuracy = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="balanced_accuracy")
print("balanced_accuracy",balanced_accuracy)

recall = cross_val_score(estimator = clf, X = X_train, y = y_train, cv = 5,scoring="recall_macro")
print("recall",recall)

print("Accuracy cross validation: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy = float(cm.diagonal().sum())/len(y_test)
print("\nAccuracy Of SVM For The Given Dataset : ", accuracy)

accuracy_score(y_test,y_pred)

print(classification_report(y_test, y_pred))

y_pred_proba= clf.predict_proba(X_test)

print("y_pred: ",y_pred_proba)
print(roc_auc_score(y_test,y_pred_proba,multi_class='ovr'))